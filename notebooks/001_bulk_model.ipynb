{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/geroldcsendes/miniconda3/envs/scgpt_2_clone/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Literal\n",
    "\n",
    "from gears import PertData\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred: np.ndarray, data: pd.DataFrame, de_target: bool):\n",
    "    ctrl = data['ctrl_mean']\n",
    "    assert pred.shape[1] == ctrl.shape[0]\n",
    "\n",
    "    corrs = []\n",
    "    for i in range(pred.shape[0]):\n",
    "        _pred = pred[i]\n",
    "        _target = data['test_y'].iloc[i].values\n",
    "        if not de_target:\n",
    "            _pred = _pred - ctrl\n",
    "            _target = _target - ctrl\n",
    "        r = pearsonr(_pred, _target)[0]\n",
    "        corrs.append(r)\n",
    "\n",
    "    print(f\"Mean correlation: {np.mean(corrs)}\")\n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_go_features(is_norman: bool, perturbation_list: list, go_pca: pd.DataFrame):\n",
    "\n",
    "    GO_NULL_STR = \"NULL\"\n",
    "    go_pca = go_pca.copy()\n",
    "    pert_list = [i.replace(\"ctrl+\", \"\").replace(\"+ctrl\", \"\") for i in perturbation_list]\n",
    "    if is_norman:\n",
    "        # insert NULL as a row to the go_pca with 0 values\n",
    "        go_pca.loc[GO_NULL_STR] = 0\n",
    "\n",
    "        pert1 = []\n",
    "        pert2 = []\n",
    "        for i in pert_list:\n",
    "            if '+' in i:\n",
    "                p1, p2 = i.split('+')\n",
    "                pert1.append(p1)\n",
    "                pert2.append(p2)\n",
    "            else:\n",
    "                pert1.append(i)\n",
    "                pert2.append('NULL')\n",
    "        \n",
    "        go_features = go_pca.loc[pert1].values - go_pca.loc[pert2].values\n",
    "\n",
    "    else:\n",
    "        go_features = go_pca.loc[pert_list]\n",
    "    return go_features\n",
    "\n",
    "def create_bulk_data(pert_data, go_pca_, is_norman: bool, de_target: bool):\n",
    "\n",
    "    data_splits = deepcopy(pert_data.set2conditions)\n",
    "\n",
    "    train_samples = data_splits['train'] + data_splits['val']\n",
    "    train_samples.remove('ctrl')\n",
    "    test_samples = data_splits['test']\n",
    "\n",
    "    assert 'ctrl' not in test_samples, \"ctrl in test samples\"\n",
    "\n",
    "    data = pert_data.adata.to_df()\n",
    "    data['condition'] = pert_data.adata.obs['condition']\n",
    "    data = data.groupby('condition').mean()\n",
    "\n",
    "    train_x = _get_go_features(is_norman, train_samples, go_pca_)\n",
    "    train_y = data.loc[train_samples].copy()\n",
    "    test_x = _get_go_features(is_norman, test_samples, go_pca_)\n",
    "    test_y = data.loc[test_samples].copy()\n",
    "    ctrl_mean = data.loc['ctrl'].values\n",
    "\n",
    "    if de_target:\n",
    "        train_y = train_y - ctrl_mean\n",
    "        test_y = test_y - ctrl_mean\n",
    "\n",
    "    assert train_x.shape[0] == train_y.shape[0], \"Train shapes do not match\"\n",
    "    assert test_x.shape[0] == test_y.shape[0], \"Test shapes do not match\"\n",
    "    assert train_x.shape[1] == test_x.shape[1], \"Train and test features do not match\"\n",
    "    assert train_y.shape[1] == test_y.shape[1], \"Train and test targets do not match\"\n",
    "    assert ctrl_mean.shape[0] == train_y.shape[1] == test_y.shape[1], \"ctrl mean shape does not match\"\n",
    "    assert ctrl_mean.ndim == 1, \"ctrl mean is not 1D\"\n",
    "\n",
    "    print(\"Number of training samples: \", train_x.shape[0])\n",
    "    print(\"Number of test samples: \", test_x.shape[0])\n",
    "\n",
    "    return dict(\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        test_x=test_x,\n",
    "        test_y=test_y,\n",
    "        ctrl_mean=ctrl_mean,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "go = pd.read_csv(\"data/go_v1.csv\", index_col=0)\n",
    "\n",
    "pca_256 = PCA(n_components=256, random_state=0)\n",
    "go_pca = pca_256.fit_transform(go)\n",
    "go_pca = pd.DataFrame(go_pca, index=go.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adamson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:22\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data_adamson = PertData(\"./data/\")\n",
    "pert_data_adamson.load(data_name='adamson')\n",
    "pert_data_adamson.prepare_split(split=\"simulation\", seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  64\n",
      "Number of test samples:  22\n"
     ]
    }
   ],
   "source": [
    "adamson_data = create_bulk_data(\n",
    "    pert_data=pert_data_adamson,\n",
    "    go_pca_=go_pca,\n",
    "    is_norman=False,\n",
    "    de_target=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=300, n_jobs=-1, verbose=True)\n",
    "rf.fit(adamson_data['train_x'], adamson_data['train_y'])\n",
    "adamson_pred = rf.predict(adamson_data['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.7585744776781169\n"
     ]
    }
   ],
   "source": [
    "adamson_res = eval(pred=adamson_pred, data=adamson_data, de_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.7083887968406702\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "en.fit(adamson_data['train_x'], adamson_data['train_y'])\n",
    "adamson_pred_en = en.predict(adamson_data['test_x'])\n",
    "\n",
    "adamson_res_en = eval(pred=adamson_pred_en, data=adamson_data, de_target=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  64\n",
      "Number of test samples:  22\n"
     ]
    }
   ],
   "source": [
    "adamson_data_de = create_bulk_data(\n",
    "    pert_data=pert_data_adamson,\n",
    "    go_pca_=go_pca,\n",
    "    is_norman=False,\n",
    "    de_target=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(adamson_data['train_x'], adamson_data_de['train_x'])\n",
    "np.testing.assert_array_equal(adamson_data['test_x'], adamson_data_de['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "rf_de = RandomForestRegressor(n_estimators=300, n_jobs=-1, verbose=True)\n",
    "rf_de.fit(adamson_data_de['train_x'], adamson_data_de['train_y'])\n",
    "adamson_pred_de = rf_de.predict(adamson_data_de['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.748541470661813\n"
     ]
    }
   ],
   "source": [
    "adamson_res_de = eval(pred=adamson_pred_de, data=adamson_data_de, de_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.7083887968629446\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "en.fit(adamson_data_de['train_x'], adamson_data_de['train_y'])\n",
    "adamson_pred_de = en.predict(adamson_data_de['test_x'])\n",
    "\n",
    "adamson_res_en_de = eval(pred=adamson_pred_de, data=adamson_data_de, de_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:9\n",
      "combo_seen1:52\n",
      "combo_seen2:18\n",
      "unseen_single:37\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data_norman = PertData(\"./data/\")\n",
    "pert_data_norman.load(data_path='data/norman/')\n",
    "pert_data_norman.prepare_split(split=\"simulation\", seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  167\n",
      "Number of test samples:  116\n"
     ]
    }
   ],
   "source": [
    "norman_data = create_bulk_data(\n",
    "    pert_data=pert_data_norman,\n",
    "    go_pca_=go_pca,\n",
    "    is_norman=True,\n",
    "    de_target=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   26.4s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=300, n_jobs=-1, verbose=True)\n",
    "rf.fit(norman_data['train_x'], norman_data['train_y'])\n",
    "norman_pred = rf.predict(norman_data['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.5887369891329108\n"
     ]
    }
   ],
   "source": [
    "norman_res = eval(pred=norman_pred, data=norman_data, de_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.5627775263133756\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "en.fit(norman_data['train_x'], norman_data['train_y'])\n",
    "norman_pred_en = en.predict(norman_data['test_x'])\n",
    "\n",
    "norman_res_en = eval(pred=norman_pred_en, data=norman_data, de_target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DE target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  167\n",
      "Number of test samples:  116\n"
     ]
    }
   ],
   "source": [
    "norman_data_de = create_bulk_data(\n",
    "    pert_data=pert_data_norman,\n",
    "    go_pca_=go_pca,\n",
    "    is_norman=True,\n",
    "    de_target=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   26.4s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "rf_de = RandomForestRegressor(n_estimators=300, n_jobs=-1, verbose=True)\n",
    "rf_de.fit(norman_data_de['train_x'], norman_data_de['train_y'])\n",
    "norman_pred_de = rf_de.predict(norman_data_de['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.5926644181130665\n"
     ]
    }
   ],
   "source": [
    "norman_res_de = eval(pred=norman_pred_de, data=norman_data_de, de_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.5627775263789643\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "en.fit(norman_data_de['train_x'], norman_data_de['train_y'])\n",
    "norman_pred_de = en.predict(norman_data_de['test_x'])\n",
    "\n",
    "norman_res_en_de = eval(pred=norman_pred_de, data=norman_data_de, de_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:273\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data_replogle = PertData(\"../data/\")\n",
    "pert_data_replogle.load(data_path='data/replogle_k562_essential/')\n",
    "pert_data_replogle.prepare_split(split=\"simulation\", seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  819\n",
      "Number of test samples:  273\n"
     ]
    }
   ],
   "source": [
    "replogle_data = create_bulk_data(\n",
    "    pert_data=pert_data_replogle,\n",
    "    go_pca_=go_pca,\n",
    "    is_norman=False,\n",
    "    de_target=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.9min finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 300 out of 300 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=300, n_jobs=-1, verbose=True)\n",
    "rf.fit(replogle_data['train_x'], replogle_data['train_y'])\n",
    "replogle_pred = rf.predict(replogle_data['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.49883955611493164\n"
     ]
    }
   ],
   "source": [
    "replogle_res = eval(pred=replogle_pred, data=replogle_data, de_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.3732669244932551\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "en.fit(replogle_data['train_x'], replogle_data['train_y'])\n",
    "replogle_pred_en = en.predict(replogle_data['test_x'])\n",
    "\n",
    "replogle_res_en = eval(pred=replogle_pred_en, data=replogle_data, de_target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DE target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  819\n",
      "Number of test samples:  273\n"
     ]
    }
   ],
   "source": [
    "replogle_data_de = create_bulk_data(\n",
    "    pert_data=pert_data_replogle,\n",
    "    go_pca_=go_pca,\n",
    "    is_norman=False,\n",
    "    de_target=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.9min finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 300 out of 300 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "rf_de = RandomForestRegressor(n_estimators=300, n_jobs=-1, verbose=True)\n",
    "rf_de.fit(replogle_data_de['train_x'], replogle_data_de['train_y'])\n",
    "replogle_pred_de = rf_de.predict(replogle_data_de['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.499104525175305\n"
     ]
    }
   ],
   "source": [
    "replogle_res_de = eval(pred=replogle_pred_de, data=replogle_data_de, de_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean correlation: 0.3732669244862345\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet(alpha=1, l1_ratio=0.5)\n",
    "en.fit(replogle_data_de['train_x'], replogle_data_de['train_y'])\n",
    "replogle_pred_de = en.predict(replogle_data_de['test_x'])\n",
    "\n",
    "replogle_res_en_de = eval(pred=replogle_pred_de, data=replogle_data_de, de_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt2_clone",
   "language": "python",
   "name": "scgpt2_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
